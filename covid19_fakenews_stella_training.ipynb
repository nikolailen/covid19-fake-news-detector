{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "view-in-github",
   "metadata": {
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/Anerol18/Fake_News_Detector_NLP_DeepLearning_Project/blob/main/fakenews_project_october_version_stella_nl_V5__from_lorena_with_cv.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b78069c6-04c0-470a-b491-4f3948c21114",
   "metadata": {
    "id": "b78069c6-04c0-470a-b491-4f3948c21114"
   },
   "source": [
    "# Environment setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "requirements_install_cell",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install project dependencies (run once per fresh runtime)\\n",
    "%pip install -r requirements.txt\\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "euiMurqEFA7l",
   "metadata": {
    "id": "euiMurqEFA7l"
   },
   "outputs": [],
   "source": [
    "# Environment setting for Google Colab\n",
    "#!pip install transformers sentence-transformers tqdm\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import copy\n",
    "import random\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_curve, auc\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import normalize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfONY_eY6GAA",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bfONY_eY6GAA",
    "outputId": "36288538-6660-41e7-f910-7c03d7f0220b"
   },
   "outputs": [],
   "source": [
    "# Optional flash-attn install + environment diagnostics\n",
    "import importlib\n",
    "import platform\n",
    "import subprocess\n",
    "import sys\n",
    "from importlib.metadata import PackageNotFoundError, version\n",
    "\n",
    "def pkg_version(name):\n",
    "    try:\n",
    "        return version(name)\n",
    "    except PackageNotFoundError:\n",
    "        return 'not installed'\n",
    "\n",
    "print('Python:', sys.version.split()[0])\n",
    "print('Platform:', platform.platform())\n",
    "print('Torch:', torch.__version__)\n",
    "print('CUDA available:', torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print('CUDA device:', torch.cuda.get_device_name(0))\n",
    "\n",
    "for pkg in ['numpy', 'pandas', 'transformers', 'scikit-learn', 'matplotlib', 'seaborn', 'tqdm']:\n",
    "    print(f'{pkg}: {pkg_version(pkg)}')\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    try:\n",
    "        import flash_attn  # noqa: F401\n",
    "        print('flash-attn already installed')\n",
    "    except Exception:\n",
    "        print('flash-attn not installed. Attempting optional install...')\n",
    "        cmd = [sys.executable, '-m', 'pip', 'install', 'flash-attn', '--no-build-isolation']\n",
    "        result = subprocess.run(cmd, capture_output=True, text=True)\n",
    "        if result.returncode == 0:\n",
    "            print('flash-attn install succeeded')\n",
    "        else:\n",
    "            print('flash-attn install failed; continuing without it')\n",
    "            print(result.stderr[-800:])\n",
    "else:\n",
    "    print('Skipping flash-attn install because CUDA is not available')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "U4osI_YG3vUq",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "U4osI_YG3vUq",
    "outputId": "d9e56c80-2605-4c2a-e612-5134f8264ce9"
   },
   "outputs": [],
   "source": [
    "# Select compute device with deterministic precedence: CUDA > MPS > CPU\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(f\"Using CUDA device: {torch.cuda.get_device_name(0)}\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    print(\"Using MPS device\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Using CPU device\")\n",
    "\n",
    "print(f\"Device selected: {device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7f07b50-8f29-4475-8376-728ca909daf6",
   "metadata": {
    "id": "c7f07b50-8f29-4475-8376-728ca909daf6"
   },
   "outputs": [],
   "source": [
    "# Setting seed:\n",
    "def set_seed_fun(seed_number: int):\n",
    "    random.seed(seed_number)\n",
    "    np.random.seed(seed_number)\n",
    "    torch.manual_seed(seed_number)\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed_number)\n",
    "        torch.cuda.manual_seed_all(seed_number)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed_fun(42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tUaf6SRS53AH",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 565,
     "referenced_widgets": [
      "c246260043974ce8876460d4d1e4dab3",
      "e8e6077c94904ed7981346bdaa83673e",
      "5bf341e4ceed4bb88a0a59f9ca998afe",
      "372d0091dfda46e49b55d8e7d9194b55",
      "7f0732a11aec46ba8eff695fd12204ac",
      "b59bdd54762949e7b65ba9caca202975",
      "44ef86322807476a991d4a6c2da02e4c",
      "796ee41d69dc4d7588cd5f92395029bc",
      "04313ff4a596405dbf4612e033fa9033",
      "294e7e8403e3453a851dbdf762ab613f",
      "f62a89949af5452583fc2bf378319e11",
      "8a2fad9d68804abca5ae5602680fb8d7",
      "c96ac3083ed04aa092f31810d00b0bce",
      "abf8586f865e420fab0d45c38cc5f5d6",
      "c333f0d1ab9f4a00b0373fa17f7df4cd",
      "d8767331746948938040fdf3326c9f93",
      "a7bc501b376446eaa842b9404d7156e5",
      "0f3644093f054a3cb1809d0701151d1e",
      "3f2b33ee1419464db22ef7d18406cf84",
      "7d94385a7b324af1b9fbf5bf29f1f31d",
      "3c1bbd3d3eb84f5ca0d46cc4e503ce5b",
      "7e24a5b344f9400da66d6f45368f93bb",
      "49fe937917cc4d3d89d6d8c02e5c2daf",
      "3068bc0f4d16462fb2b8dd67d77d08b2",
      "d6d49dd564aa4449a6a91034cd6d2450",
      "08b87f1fbc704c7bbe2365954dd449ca",
      "97982fbe6cce4689943ff1b3abb8ab36",
      "59345ea0911a475c9a46d889fe73f3cd",
      "e5bcbcce070d4080ac927177aae5e25e",
      "77e91c4bf36e4d15b681ab61289156cd",
      "9419f6b6ed5c4dc3a3fc6cee09232fc6",
      "001cb5e3af824afe83e2e38d08560b88",
      "e86abeac355f4b8ab5f86b44587a5d9e",
      "5dafd667c8274b9798bd2da5dbed4937",
      "c300952cc9e94078bb67dd20fab15039",
      "7eacabc754e84bbcbec25f60259d9a2b",
      "56bb7107e5e64352946bf93c8cc5498c",
      "89ac257448e74ce58fcb69f81c28bbb7",
      "65de45d148a5445d853aa399486f9014",
      "9c97ffdaae97423798ab49dc84c97ed3",
      "66a52ec180dd41fbb8796ee37c73f0e5",
      "a7f5985634f84aa98a1a6c0e9020be4d",
      "5de4cea8b6e24ad498ff9c65676712e6",
      "9cfd34f915684cc5a0527ca5e4104d43",
      "9384aaa529a24b3c8d472f58f68e4cb2",
      "06089a7ee9c2408bbfa1e81b91c804ce",
      "e89f30bde9724885905d15da92105483",
      "bc03390d680d495b9aa1da8ee7331502",
      "3a4f0115ee70472bba0b1e4303f44b24",
      "8401dd0f61824418a7bdf6fa88766836",
      "f7d6cdebefcb45c481ea0c883260b194",
      "c5d14d51d64840c9ba85cdbab5d11824",
      "84f33e8d23444e7792b799ea2638ac61",
      "47e76e99ae15402ba7e36c25ac33e08f",
      "1d22ca39fde94145a0f8db6be1fda81b",
      "b99dae627ef34c68b460860b269f6105",
      "dd820454f9dd4487848ed95908a8f55b",
      "cb49ea5ce744498991bdb7a71c79a3bb",
      "35eb3e05d2ec45d4a081a2a485b2127a",
      "e449173ee6884d749c9e308b18253f23",
      "63b773df949f47389b82d28686eda12a",
      "94d8b30e9aa94b92b3a094424a97f5bc",
      "4f66432a53f94d72a6b80b34f80565b0",
      "2c06f4bea17b44438649ecd59d789c4f",
      "3faed874195643f58316d71518b638b3",
      "e3861e3a6f2b4192b64fa6809169e47e",
      "fcaa739bb4524cd192007f63b24fd689",
      "cfff3212a2384cfeb32dbe4e116990cc",
      "3b754c48e43247fc9b421e1454a567e7",
      "ad49c4a4f49a46c697044f9376aee29d",
      "53257c0e477a4ff2be5635490026a42d",
      "da05a924ea8148358d693511ce4c57a9",
      "241ebd784d5b4e799c837ecc03dd3749",
      "49579c14057f4cab96892705a955c1ab",
      "5c4b953499bd48f28a77f9b81a2c2201",
      "3c477057e8b8478cadf83116a3be7ec9",
      "353652c1701c4f4882dbf9676a836a77",
      "4578fcc2681b4b099fb132ee27d5d4ed",
      "824f33252f0f4a71862000cd8dfeb8ee",
      "4d35340985b649a786f72e39eb1b50f4",
      "eae5d8e89d9f421cabf1afd86f1dd8f9",
      "ae53643f060f49a0b5bf40de33f3e36e",
      "2c1551fd21f54b2fa6931ca49e4c563f",
      "8465e55a93b8413f94d15f283b719e0e",
      "edfc8159b2e940b5a4a9531693a277d1",
      "118656e688c5497895dc60b2ec84cdc9",
      "7104ff2969694573a68042249e724e66",
      "880317204b4f400bae0991b83bb23281",
      "782faef81a9241ac87efbb639a8e2d61",
      "a3779fde4ce5435dbc39591e8e670492",
      "522ee4ebc49f4798a4b82f0d46aa88b6",
      "9074a098815440e29d287f9fa208bd00",
      "8de986c8542c4c3b8064d37092e57fa7",
      "3e3b1bf184194f6faab61f656ae3439f",
      "5dc1f0fded5f4ae992ed340b0401d359",
      "1f6bef90298f4a0e9bc5d559ec80eea7",
      "fe905a62f3964cf98322bdcb2666e258",
      "f32bc421f84e43ae92f95326f31bafe9",
      "b821ccd68ecd49df8c62ebb77004c08f",
      "e1d088a2a241498bbafd3744d87df012",
      "bc5318792b7840d69812d6733a22eff8",
      "c0deca87cbbe4de883dafd8bb7268e59",
      "c46f738c0afb486daeb83e5ce431e732",
      "f9623c3e2a2a420497e73c0ffaf1e5c3",
      "fab5f62da8de461b819a55257c84e2d2",
      "2354811ff51e42d6a2b41497a8c0c5a6",
      "0fd11f5b13e242c789681e2f09580629",
      "392e83407d3f472ba4be544357e046a6",
      "00a2e0f9b38a47ca8ca57f94efbd2e55",
      "ab96cbaa280c4e23b666e2677665a665"
     ]
    },
    "id": "tUaf6SRS53AH",
    "outputId": "1196c542-f315-4138-8ac2-1f45f2cebe75"
   },
   "outputs": [],
   "source": [
    "# Load the embedding model (dunzhang/stella_en_1.5B_v5)\n",
    "model_name = \"dunzhang/stella_en_1.5B_v5\"\n",
    "model = AutoModel.from_pretrained(model_name, trust_remote_code=True).to(device).eval()\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99e07a94-fbf3-4d20-84bc-822e61682eac",
   "metadata": {
    "id": "99e07a94-fbf3-4d20-84bc-822e61682eac"
   },
   "outputs": [],
   "source": [
    "# Import dataset (prefer local file, fallback to remote URL)\n",
    "from pathlib import Path\n",
    "import hashlib\n",
    "\n",
    "local_dataset_path = Path('final_combined_dataset.csv')\n",
    "snapshot_hash_path = Path('final_combined_dataset.sha256')\n",
    "remote_dataset_url = 'https://raw.githubusercontent.com/Anerol18/Fake_News_Detector_NLP_DeepLearning_Project/main/final_combined_dataset.csv'\n",
    "\n",
    "if local_dataset_path.exists():\n",
    "    print(f'Loading dataset from local file: {local_dataset_path}')\n",
    "    df = pd.read_csv(local_dataset_path)\n",
    "\n",
    "    current_hash = hashlib.sha256(local_dataset_path.read_bytes()).hexdigest()\n",
    "    print(f'Local dataset SHA256: {current_hash}')\n",
    "\n",
    "    if snapshot_hash_path.exists():\n",
    "        expected_hash = snapshot_hash_path.read_text(encoding='utf-8').strip().split()[0]\n",
    "        if current_hash != expected_hash:\n",
    "            raise ValueError(\n",
    "                f'Dataset hash mismatch. Expected {expected_hash}, got {current_hash}. '\n",
    "                'Update final_combined_dataset.sha256 only if this dataset update is intentional.'\n",
    "            )\n",
    "        print('Dataset hash matches pinned snapshot')\n",
    "    else:\n",
    "        print('No pinned snapshot hash file found (final_combined_dataset.sha256)')\n",
    "else:\n",
    "    print('Local dataset not found. Falling back to remote URL.')\n",
    "    df = pd.read_csv(remote_dataset_url)\n",
    "\n",
    "required_columns = {'Text', 'Label'}\n",
    "missing_columns = required_columns - set(df.columns)\n",
    "if missing_columns:\n",
    "    raise ValueError(f'Dataset is missing required columns: {missing_columns}')\n",
    "\n",
    "# Basic cleanup for robust downstream processing\n",
    "df = df.dropna(subset=['Text', 'Label']).copy()\n",
    "df['Text'] = df['Text'].astype(str).str.strip()\n",
    "df['Label'] = df['Label'].astype(str).str.strip().str.lower()\n",
    "df = df[df['Text'] != ''].reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46d4ab2b-ee2a-4ce9-9b20-c457b0b0ac2c",
   "metadata": {
    "id": "46d4ab2b-ee2a-4ce9-9b20-c457b0b0ac2c"
   },
   "outputs": [],
   "source": [
    "# Prepare data\n",
    "X = df['Text'].values.astype(str)\n",
    "y = (df['Label'] == 'fake').astype(int).values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a7d200-6a4e-4bb1-a166-05da53b3ab10",
   "metadata": {
    "id": "81a7d200-6a4e-4bb1-a166-05da53b3ab10"
   },
   "outputs": [],
   "source": [
    "# Split the data into training, validation, and test sets\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(\n",
    "    X,\n",
    "    y,\n",
    "    test_size=0.3,\n",
    "    random_state=42,\n",
    "    stratify=y,\n",
    ")\n",
    "X_val, X_test, y_val, y_test = train_test_split(\n",
    "    X_temp,\n",
    "    y_temp,\n",
    "    test_size=0.5,\n",
    "    random_state=42,\n",
    "    stratify=y_temp,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dc6ae47-93f1-40a0-9295-3c75928120c7",
   "metadata": {
    "id": "6dc6ae47-93f1-40a0-9295-3c75928120c7"
   },
   "source": [
    "# Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7Nubkly8ocs",
   "metadata": {
    "id": "b7Nubkly8ocs"
   },
   "outputs": [],
   "source": [
    "# Modified function without dimension reduction\n",
    "def generate_stella_embeddings(texts, tokenizer, model, batch_size=32):\n",
    "    embeddings = []\n",
    "\n",
    "    for i in tqdm(range(0, len(texts), batch_size), desc=\"Generating Embeddings\"):\n",
    "        batch_texts = texts[i:i + batch_size]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            inputs = tokenizer(batch_texts, padding=\"longest\", truncation=True, max_length=512, return_tensors=\"pt\").to(device)\n",
    "            attention_mask = inputs[\"attention_mask\"]\n",
    "            outputs = model(**inputs)[0]\n",
    "            last_hidden = outputs.masked_fill(~attention_mask[..., None].bool(), 0.0)\n",
    "            embeddings_batch = last_hidden.sum(dim=1) / attention_mask.sum(dim=1)[..., None]\n",
    "            embeddings_batch = normalize(embeddings_batch.cpu().numpy())\n",
    "\n",
    "            embeddings.append(embeddings_batch)\n",
    "\n",
    "    return np.vstack(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "befa178b-cbc7-4d7f-9c96-5cbb828470ee",
   "metadata": {
    "id": "befa178b-cbc7-4d7f-9c96-5cbb828470ee"
   },
   "outputs": [],
   "source": [
    "#####################################\n",
    "# benchmark beginning for embedding #\n",
    "#####################################\n",
    "time_start_embed = time.perf_counter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1Z8m1kVFnTQ",
   "metadata": {
    "id": "a1Z8m1kVFnTQ"
   },
   "outputs": [],
   "source": [
    "# Initialize the Vertex AI TextEmbeddingModel\n",
    "# embedding_model = TextEmbeddingModel.from_pretrained(\"text-embedding-004\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "UF22BrrY79RS",
   "metadata": {
    "id": "UF22BrrY79RS"
   },
   "outputs": [],
   "source": [
    "# Ensure data is in the correct format\n",
    "X_train = X_train.tolist() if isinstance(X_train, np.ndarray) else X_train\n",
    "X_val = X_val.tolist() if isinstance(X_val, np.ndarray) else X_val\n",
    "X_test = X_test.tolist() if isinstance(X_test, np.ndarray) else X_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "kUiUJO8c9Udz",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kUiUJO8c9Udz",
    "outputId": "287e2dec-f105-4357-c968-be132a844092"
   },
   "outputs": [],
   "source": [
    "# Generate embeddings for the train, validation, and test sets\n",
    "X_train_embeddings = generate_stella_embeddings(X_train, tokenizer, model)\n",
    "X_val_embeddings = generate_stella_embeddings(X_val, tokenizer, model)\n",
    "X_test_embeddings = generate_stella_embeddings(X_test, tokenizer, model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb68c53d-0a6c-4f2e-99f6-1c85d0d26ef1",
   "metadata": {
    "id": "eb68c53d-0a6c-4f2e-99f6-1c85d0d26ef1"
   },
   "outputs": [],
   "source": [
    "#####################################\n",
    "# benchmark ending for embedding    #\n",
    "#####################################\n",
    "time_end_embed = time.perf_counter()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc999217-4072-46d1-8e44-b05cd901fa66",
   "metadata": {
    "id": "fc999217-4072-46d1-8e44-b05cd901fa66"
   },
   "source": [
    "# Training part"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7acb5108-17ca-499e-8ecd-b4933f1b2b18",
   "metadata": {
    "id": "7acb5108-17ca-499e-8ecd-b4933f1b2b18"
   },
   "source": [
    "## Class functions\n",
    "\n",
    "Definition of functions to:\n",
    "- Transform a data set into the good format\n",
    "- create a simple neural network architecture\n",
    "- create a funtion to transform seconds into a list of (hours, minutes, seconds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "423bc231-7992-40a8-aaaa-cc7920a72ceb",
   "metadata": {
    "id": "423bc231-7992-40a8-aaaa-cc7920a72ceb"
   },
   "outputs": [],
   "source": [
    "# Define a Dataset class for PyTorch\n",
    "class NewsDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.y)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "         # Ensure X is a numeric tensor\n",
    "        text_tensor = torch.tensor(self.X[idx], dtype=torch.float32)  # Make sure this is float\n",
    "        label_tensor = torch.tensor(self.y[idx], dtype=torch.long)  # Labels should be long for classification\n",
    "        return text_tensor, label_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bpqcOYYqZ1fv",
   "metadata": {
    "id": "bpqcOYYqZ1fv"
   },
   "outputs": [],
   "source": [
    "# input_size = 1536 / 768 / 384 / 192"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c2b1910-63be-4ca7-ab7d-f441ad81c576",
   "metadata": {
    "id": "8c2b1910-63be-4ca7-ab7d-f441ad81c576"
   },
   "outputs": [],
   "source": [
    "# Define a simple neural network\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self, input_size):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.fc0 = nn.Linear(input_size, 3072)\n",
    "        self.dropout0 = nn.Dropout(p=0.6)\n",
    "        self.relu0 = nn.ReLU()\n",
    "        self.fc01 = nn.Linear(3072, 3072)\n",
    "        self.dropout01 = nn.Dropout(p=0.6)\n",
    "        self.relu01 = nn.ReLU()\n",
    "        self.fc1 = nn.Linear(3072, 768)\n",
    "        self.dropout1 = nn.Dropout(p=0.6)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        #self.fc11 = nn.Linear(768, 768)\n",
    "        #self.dropout11 = nn.Dropout(p=0.6)\n",
    "        #self.relu11 = nn.ReLU()\n",
    "        #self.fc2 = nn.Linear(768, 384)\n",
    "        #self.dropout2 = nn.Dropout(p=0.6)\n",
    "        #self.relu2 = nn.ReLU()\n",
    "        #self.fc21 = nn.Linear(384, 384)\n",
    "        #self.dropout21 = nn.Dropout(p=0.6)\n",
    "        #self.relu21 = nn.ReLU()\n",
    "        self.fc3 = nn.Linear(768, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc0(x)\n",
    "        x = self.dropout0(x)\n",
    "        x = self.relu0(x)\n",
    "        x = self.fc01(x)\n",
    "        x = self.dropout01(x)\n",
    "        x = self.relu01(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.dropout1(x)\n",
    "        x = self.relu1(x)\n",
    "        #x = self.fc11(x)\n",
    "        #x = self.dropout1(x)\n",
    "        #x = self.relu1(x)\n",
    "        #x = self.fc2(x)\n",
    "        #x = self.dropout2(x)\n",
    "        #x = self.relu2(x)\n",
    "        #x = self.fc21(x)\n",
    "        #x = self.dropout21(x)\n",
    "        #x = self.relu21(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff2fedd9-c85c-4144-8dd5-139af39d891d",
   "metadata": {
    "id": "ff2fedd9-c85c-4144-8dd5-139af39d891d"
   },
   "outputs": [],
   "source": [
    "def sec2hms(ss):\n",
    "\t(hh, ss)=divmod(ss, 3600)\n",
    "\t(mm, ss)=divmod(ss, 60)\n",
    "\treturn (hh, mm, ss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5a1eed2-7a53-4921-9d9f-cb35848dedbf",
   "metadata": {
    "id": "f5a1eed2-7a53-4921-9d9f-cb35848dedbf"
   },
   "source": [
    "## Training function\n",
    "\n",
    "Definition of the training function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c424347-5b2d-42cd-829a-1a36e6dcf742",
   "metadata": {
    "id": "3c424347-5b2d-42cd-829a-1a36e6dcf742"
   },
   "outputs": [],
   "source": [
    "# Function to train the model with stratified K-fold cross-validation\n",
    "\n",
    "def train_model_cv(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    input_size,\n",
    "    n_splits=5,\n",
    "    num_epochs=40,\n",
    "    batch_size=64,\n",
    "    lr=2e-5,\n",
    "    patience=2,\n",
    "):\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    weight_decay_values = [0, 0.0001, 0.001, 0.01, 0.1]\n",
    "    results = {}\n",
    "\n",
    "    for weight_decay in weight_decay_values:\n",
    "        print(f\"\\n=== Weight decay: {weight_decay} ===\")\n",
    "        fold_results = []\n",
    "\n",
    "        for fold, (train_idx, val_idx) in enumerate(skf.split(X_train, y_train), start=1):\n",
    "            print(f\"Training on fold {fold}/{n_splits}\")\n",
    "\n",
    "            assert len(set(train_idx).intersection(set(val_idx))) == 0, \"train/val indices overlap\"\n",
    "            assert len(train_idx) + len(val_idx) == len(X_train), \"fold partition mismatch\"\n",
    "\n",
    "            X_fold_train = X_train[train_idx]\n",
    "            y_fold_train = y_train[train_idx]\n",
    "            X_fold_val = X_train[val_idx]\n",
    "            y_fold_val = y_train[val_idx]\n",
    "\n",
    "            train_dataset = NewsDataset(X_fold_train, y_fold_train)\n",
    "            val_dataset = NewsDataset(X_fold_val, y_fold_val)\n",
    "\n",
    "            train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "            val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "            model = SimpleNN(input_size).to(device)\n",
    "            criterion = nn.CrossEntropyLoss()\n",
    "            optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "            best_val_loss = float(\"inf\")\n",
    "            best_val_accuracy = 0.0\n",
    "            best_epoch = 0\n",
    "            best_state_dict = copy.deepcopy(model.state_dict())\n",
    "            patience_counter = 0\n",
    "\n",
    "            train_losses, val_losses = [], []\n",
    "            train_accuracies, val_accuracies = [], []\n",
    "\n",
    "            for epoch in range(num_epochs):\n",
    "                model.train()\n",
    "                running_loss = 0.0\n",
    "                train_correct = 0\n",
    "                train_total = 0\n",
    "\n",
    "                for X_batch, y_batch in train_loader:\n",
    "                    X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "\n",
    "                    optimizer.zero_grad()\n",
    "                    outputs = model(X_batch)\n",
    "                    loss = criterion(outputs, y_batch)\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "                    running_loss += loss.item()\n",
    "                    preds = outputs.argmax(dim=1)\n",
    "                    train_total += y_batch.size(0)\n",
    "                    train_correct += (preds == y_batch).sum().item()\n",
    "\n",
    "                avg_train_loss = running_loss / max(len(train_loader), 1)\n",
    "                train_accuracy = train_correct / max(train_total, 1)\n",
    "                train_losses.append(avg_train_loss)\n",
    "                train_accuracies.append(train_accuracy)\n",
    "\n",
    "                model.eval()\n",
    "                val_loss_total = 0.0\n",
    "                val_correct = 0\n",
    "                val_total = 0\n",
    "                with torch.no_grad():\n",
    "                    for X_batch, y_batch in val_loader:\n",
    "                        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "                        outputs = model(X_batch)\n",
    "                        loss = criterion(outputs, y_batch)\n",
    "                        val_loss_total += loss.item()\n",
    "\n",
    "                        preds = outputs.argmax(dim=1)\n",
    "                        val_total += y_batch.size(0)\n",
    "                        val_correct += (preds == y_batch).sum().item()\n",
    "\n",
    "                avg_val_loss = val_loss_total / max(len(val_loader), 1)\n",
    "                val_accuracy = val_correct / max(val_total, 1)\n",
    "                val_losses.append(avg_val_loss)\n",
    "                val_accuracies.append(val_accuracy)\n",
    "\n",
    "                print(\n",
    "                    f\"Epoch {epoch + 1}/{num_epochs} | \"\n",
    "                    f\"Train Loss: {avg_train_loss:.6f}, Train Acc: {train_accuracy:.6f}, \"\n",
    "                    f\"Val Loss: {avg_val_loss:.6f}, Val Acc: {val_accuracy:.6f}\"\n",
    "                )\n",
    "\n",
    "                if avg_val_loss < best_val_loss:\n",
    "                    best_val_loss = avg_val_loss\n",
    "                    best_val_accuracy = val_accuracy\n",
    "                    best_epoch = epoch + 1\n",
    "                    best_state_dict = copy.deepcopy(model.state_dict())\n",
    "                    patience_counter = 0\n",
    "                else:\n",
    "                    patience_counter += 1\n",
    "\n",
    "                if patience_counter >= patience:\n",
    "                    print(\"Early stopping\")\n",
    "                    break\n",
    "\n",
    "            model.load_state_dict(best_state_dict)\n",
    "\n",
    "            fold_results.append(\n",
    "                {\n",
    "                    \"train_losses\": train_losses,\n",
    "                    \"val_losses\": val_losses,\n",
    "                    \"train_accuracies\": train_accuracies,\n",
    "                    \"val_accuracies\": val_accuracies,\n",
    "                    \"best_val_loss\": best_val_loss,\n",
    "                    \"best_val_accuracy\": best_val_accuracy,\n",
    "                    \"best_epoch\": best_epoch,\n",
    "                }\n",
    "            )\n",
    "\n",
    "        results[weight_decay] = fold_results\n",
    "\n",
    "    return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58ce1736-5492-4a67-9311-4294831030b0",
   "metadata": {
    "id": "58ce1736-5492-4a67-9311-4294831030b0"
   },
   "source": [
    "# Model improvement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7ebddb4-ac72-4d8e-9bc7-164b69619894",
   "metadata": {
    "id": "c7ebddb4-ac72-4d8e-9bc7-164b69619894"
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4511da9a-0124-4288-82a6-7a36576ad514",
   "metadata": {
    "id": "4511da9a-0124-4288-82a6-7a36576ad514"
   },
   "outputs": [],
   "source": [
    "############################################\n",
    "# benchmark beginning for Cross Validation #\n",
    "############################################\n",
    "time_start_cv = time.perf_counter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "toO_23frOXAC",
   "metadata": {
    "id": "toO_23frOXAC"
   },
   "outputs": [],
   "source": [
    "input_size = 1536  # set from stella dimensions\n",
    "\n",
    "# Quick smoke-run controls (set QUICK_RUN=False for full training)\n",
    "QUICK_RUN = False\n",
    "cv_n_splits = 3 if QUICK_RUN else 5\n",
    "cv_num_epochs = 5 if QUICK_RUN else 40\n",
    "retrain_num_epochs = 5 if QUICK_RUN else 40\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40991ebd-b228-4ebe-ae91-f750a75ce5ed",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "40991ebd-b228-4ebe-ae91-f750a75ce5ed",
    "jupyter": {
     "outputs_hidden": true
    },
    "outputId": "7a584add-a434-4dc5-d633-9ea62ad17c8e"
   },
   "outputs": [],
   "source": [
    "# Train the model\n",
    "results = train_model_cv(\n",
    "    X_train_embeddings,\n",
    "    y_train,\n",
    "    input_size,\n",
    "    n_splits=cv_n_splits,\n",
    "    num_epochs=cv_num_epochs,\n",
    "    batch_size=64,\n",
    "    lr=2e-5,\n",
    "    patience=2,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "LY3T2J4hFv05",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LY3T2J4hFv05",
    "outputId": "6f1d7db9-bae2-4060-f673-52bede0c290c"
   },
   "outputs": [],
   "source": [
    "# Initialize variables to track best metrics\n",
    "best_weight_decay = None\n",
    "best_val_loss = float('inf')\n",
    "best_val_accuracy = 0.0\n",
    "avg_best_val_losses = []\n",
    "avg_best_val_accuracies = []\n",
    "\n",
    "for wd, fold_metrics in results.items():\n",
    "    avg_best_val_loss = np.mean([fold['best_val_loss'] for fold in fold_metrics])\n",
    "    avg_best_val_accuracy = np.mean([fold['best_val_accuracy'] for fold in fold_metrics])\n",
    "\n",
    "    avg_best_val_losses.append(avg_best_val_loss)\n",
    "    avg_best_val_accuracies.append(avg_best_val_accuracy)\n",
    "\n",
    "    if avg_best_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_best_val_loss\n",
    "        best_weight_decay = wd\n",
    "        best_val_accuracy = avg_best_val_accuracy\n",
    "\n",
    "print(\n",
    "    f\"Best Weight Decay: {best_weight_decay:.6f}, \"\n",
    "    f\"Best CV Validation Loss: {best_val_loss:.6f}, \"\n",
    "    f\"Best CV Validation Accuracy: {best_val_accuracy:.6f}\"\n",
    ")\n",
    "print(\"Average Best Validation Losses: \", [\"{:.6f}\".format(item) for item in avg_best_val_losses])\n",
    "print(\"Average Best Validation Accuracies: \", [\"{:.6f}\".format(item) for item in avg_best_val_accuracies])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43732b1f-cdf3-450d-a147-0349d3fc86be",
   "metadata": {
    "id": "43732b1f-cdf3-450d-a147-0349d3fc86be"
   },
   "outputs": [],
   "source": [
    "#########################################\n",
    "# Benchmark ending for Cross Validation #\n",
    "#########################################\n",
    "time_end_cv = time.perf_counter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "XWZwWED-l27b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 556
    },
    "id": "XWZwWED-l27b",
    "outputId": "d79688b5-d033-4483-9243-b326ece9c160"
   },
   "outputs": [],
   "source": [
    "# Visualize the results\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Iterate through each weight decay and its corresponding fold metrics\n",
    "for wd, fold_metrics in results.items():\n",
    "    # Get the lengths of val_losses for each fold\n",
    "    lengths = [len(fold['val_losses']) for fold in fold_metrics]\n",
    "    # Find the minimum length\n",
    "    min_length = min(lengths)\n",
    "    # Truncate val_losses to the minimum length for consistent shapes\n",
    "    truncated_val_losses = [fold['val_losses'][:min_length] for fold in fold_metrics]\n",
    "\n",
    "    # Compute the average validation loss across all folds for each epoch using the truncated lists\n",
    "    avg_val_losses = np.mean(truncated_val_losses, axis=0)  # Average over folds\n",
    "\n",
    "    plt.plot(avg_val_losses, label=f'Weight Decay: {wd}', marker='o')  # Adding marker for better visibility\n",
    "\n",
    "plt.title('Average Validation Loss vs. Epochs for Different Weight Decay Values')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Average Validation Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)  # Add grid for better readability\n",
    "plt.ylim(bottom=0)  # Ensure y-axis starts at 0 for better visibility\n",
    "plt.savefig(\"Cross Validation on Weight Decay.png\", transparent=True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2k0cyEC-1Dhz",
   "metadata": {
    "id": "2k0cyEC-1Dhz"
   },
   "source": [
    "## Retrain the model using the best AdamW decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "YT-I6Ovw1DKS",
   "metadata": {
    "id": "YT-I6Ovw1DKS"
   },
   "outputs": [],
   "source": [
    "# Function to retrain the model using the best weight decay\n",
    "\n",
    "def retrain_with_best_decay(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    X_val,\n",
    "    y_val,\n",
    "    input_size,\n",
    "    best_weight_decay,\n",
    "    num_epochs=40,\n",
    "    batch_size=64,\n",
    "    lr=2e-5,\n",
    "    patience=2,\n",
    "):\n",
    "    train_dataset = NewsDataset(X_train, y_train)\n",
    "    val_dataset = NewsDataset(X_val, y_val)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    model = SimpleNN(input_size).to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=best_weight_decay)\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    best_epoch = 0\n",
    "    best_state_dict = copy.deepcopy(model.state_dict())\n",
    "    patience_counter = 0\n",
    "\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    train_accuracies = []\n",
    "    val_accuracies = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        train_correct = 0\n",
    "        train_total = 0\n",
    "\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(X_batch)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            preds = outputs.argmax(dim=1)\n",
    "            train_total += y_batch.size(0)\n",
    "            train_correct += (preds == y_batch).sum().item()\n",
    "\n",
    "        avg_train_loss = running_loss / max(len(train_loader), 1)\n",
    "        train_accuracy = train_correct / max(train_total, 1)\n",
    "\n",
    "        train_losses.append(avg_train_loss)\n",
    "        train_accuracies.append(train_accuracy)\n",
    "\n",
    "        model.eval()\n",
    "        val_loss_total = 0.0\n",
    "        val_correct = 0\n",
    "        val_total = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for X_batch, y_batch in val_loader:\n",
    "                X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "                outputs = model(X_batch)\n",
    "                loss = criterion(outputs, y_batch)\n",
    "                val_loss_total += loss.item()\n",
    "\n",
    "                preds = outputs.argmax(dim=1)\n",
    "                val_total += y_batch.size(0)\n",
    "                val_correct += (preds == y_batch).sum().item()\n",
    "\n",
    "        avg_val_loss = val_loss_total / max(len(val_loader), 1)\n",
    "        val_accuracy = val_correct / max(val_total, 1)\n",
    "\n",
    "        val_losses.append(avg_val_loss)\n",
    "        val_accuracies.append(val_accuracy)\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epoch + 1}/{num_epochs} | \"\n",
    "            f\"Train Loss: {avg_train_loss:.6f}, Train Acc: {train_accuracy:.6f}, \"\n",
    "            f\"Val Loss: {avg_val_loss:.6f}, Val Acc: {val_accuracy:.6f}\"\n",
    "        )\n",
    "\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            best_epoch = epoch + 1\n",
    "            best_state_dict = copy.deepcopy(model.state_dict())\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "\n",
    "        if patience_counter >= patience:\n",
    "            print(\"Early stopping\")\n",
    "            break\n",
    "\n",
    "    model.load_state_dict(best_state_dict)\n",
    "    print(f\"Restored best model checkpoint from epoch {best_epoch} with val loss {best_val_loss:.6f}\")\n",
    "\n",
    "    return model, train_losses, val_losses, train_accuracies, val_accuracies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "_ok46oQSA0S8",
   "metadata": {
    "id": "_ok46oQSA0S8"
   },
   "outputs": [],
   "source": [
    "##########################################\n",
    "# Benchmark beginning for best modeling  #\n",
    "##########################################\n",
    "time_start_model = time.perf_counter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5uAsMQenA9jc",
   "metadata": {
    "id": "5uAsMQenA9jc"
   },
   "outputs": [],
   "source": [
    "input_size = 1536"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Rx7m_Eut-jnW",
   "metadata": {
    "id": "Rx7m_Eut-jnW"
   },
   "outputs": [],
   "source": [
    "# #Coverting into int\n",
    "# label_mapping = {'real': 0, 'fake': 1}\n",
    "\n",
    "# # Convert y_train and y_val only if they are strings\n",
    "# y_train = [label_mapping.get(label, label) if isinstance(label, str) else label for label in y_train]\n",
    "# y_val = [label_mapping.get(label, label) if isinstance(label, str) else label for label in y_val]\n",
    "\n",
    "# # Ensure all elements are integers before creating tensors\n",
    "# y_train = [int(label) for label in y_train]  # Convert all elements to integers\n",
    "# y_val = [int(label) for label in y_val]  # Convert all elements to integers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87rFZ-7v-nlk",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "87rFZ-7v-nlk",
    "outputId": "3724e3a2-11e3-44a4-8764-5d68921be08b"
   },
   "outputs": [],
   "source": [
    "# Train the model with the best weight decay\n",
    "model, train_losses, val_losses, train_accuracies, val_accuracies = retrain_with_best_decay(\n",
    "    X_train_embeddings,\n",
    "    y_train,\n",
    "    X_val_embeddings,\n",
    "    y_val,\n",
    "    input_size,\n",
    "    best_weight_decay,\n",
    "    num_epochs=retrain_num_epochs,\n",
    "    batch_size=64,\n",
    "    lr=2e-5,\n",
    "    patience=2,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "AUTShmq3nWXj",
   "metadata": {
    "id": "AUTShmq3nWXj"
   },
   "outputs": [],
   "source": [
    "######################################\n",
    "# Benchmark ending for best modeling #\n",
    "######################################\n",
    "time_end_model = time.perf_counter()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97kVEkEgDbsM",
   "metadata": {
    "id": "97kVEkEgDbsM"
   },
   "source": [
    "## Evaluating\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "KkeE84sdRJxZ",
   "metadata": {
    "id": "KkeE84sdRJxZ"
   },
   "outputs": [],
   "source": [
    "# Function to evaluate the model on the test set\n",
    "\n",
    "def evaluate_model(model, X_test, y_test):\n",
    "    model.eval()\n",
    "    test_dataset = NewsDataset(X_test, y_test)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "    y_pred = []\n",
    "    y_score = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X_batch, _ in test_loader:\n",
    "            X_batch = X_batch.to(device)\n",
    "            outputs = model(X_batch)\n",
    "            probs = torch.softmax(outputs, dim=1)\n",
    "            preds = probs.argmax(dim=1)\n",
    "\n",
    "            y_pred.extend(preds.cpu().numpy())\n",
    "            y_score.extend(probs[:, 1].cpu().numpy())\n",
    "\n",
    "    return np.array(y_pred), np.array(y_score)\n",
    "\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "y_pred, y_score = evaluate_model(model, X_test_embeddings, y_test)\n",
    "\n",
    "# Metric integrity checks\n",
    "assert len(y_score) == len(y_test), \"score/prediction length mismatch\"\n",
    "assert np.all((y_score >= 0.0) & (y_score <= 1.0)), \"predicted probabilities are out of [0, 1] range\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "_6GkWHejSDzk",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 824
    },
    "id": "_6GkWHejSDzk",
    "outputId": "4b198fd7-6e9c-4c73-cba4-dde3f37e514e"
   },
   "outputs": [],
   "source": [
    "# Evaluate performance\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "class_report = classification_report(y_test, y_pred, target_names=[\"real\", \"fake\"])\n",
    "conf_matrix = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "print(f\"Accuracy: {acc:.6f}\")\n",
    "print(f\"Classification Report:\\n{class_report}\")\n",
    "print(f\"Confusion Matrix:\\n{conf_matrix}\")\n",
    "\n",
    "# Visualize the confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=[\"real\", \"fake\"], yticklabels=[\"real\", \"fake\"])\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.savefig('Confusion Matrix.png', transparent=True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "kWxAjJVkP_Lg",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 444
    },
    "id": "kWxAjJVkP_Lg",
    "outputId": "041d17a7-e588-484f-95a0-56c70e9af6c6"
   },
   "outputs": [],
   "source": [
    "# Plot the loss curves\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(range(1, len(train_losses) + 1), train_losses, label='Train Loss')\n",
    "plt.plot(range(1, len(val_losses) + 1), val_losses, label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.savefig('Loss curves.png', transparent=True)\n",
    "\n",
    "# Plot the accuracy curves\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(range(1, len(train_accuracies) + 1), train_accuracies, label='Train Accuracy')\n",
    "plt.plot(range(1, len(val_accuracies) + 1), val_accuracies, label='Validation Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.grid(True)\n",
    "plt.savefig('Accuracy curves.png', transparent=True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "akMaGh1QXXk4",
   "metadata": {
    "id": "akMaGh1QXXk4"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "print(\"Train Losses:\", train_losses)\n",
    "print(\"Validation Losses:\",val_losses)\n",
    "print(\"Train Accuracies:\", train_accuracies)\n",
    "print(\"Validation Accuracies:\", val_accuracies)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mILYNY7ajaFr",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mILYNY7ajaFr",
    "outputId": "4c54ddd2-b92b-4ad2-8420-48aaf3428322"
   },
   "outputs": [],
   "source": [
    "print('Train Losses: ', ['{:.6f}'.format(item) for item in train_losses])\n",
    "print('Validation Losses: ', ['{:.6f}'.format(item) for item in val_losses])\n",
    "print('Train Accuracies: ', ['{:.6f}'.format(item) for item in train_accuracies])\n",
    "print('Validation Accuracies: ', ['{:.6f}'.format(item) for item in val_accuracies])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30jsiSAOZdLL",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 581
    },
    "id": "30jsiSAOZdLL",
    "outputId": "3f31c6c3-e5f0-40a5-b264-a72bf4105e14"
   },
   "outputs": [],
   "source": [
    "# Plot ROC curve\n",
    "fpr, tpr, _ = roc_curve(y_test, y_score)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (AUC = {roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.savefig('ROC curve.png', transparent=True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "jH49qcm-C8QR",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jH49qcm-C8QR",
    "outputId": "69a07109-e885-49d0-db17-bf1bdb766e72"
   },
   "outputs": [],
   "source": [
    "print(f'ROC AUC: {roc_auc:.6f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ad31460-8f94-4de2-be81-aa55e8196ebb",
   "metadata": {
    "id": "7ad31460-8f94-4de2-be81-aa55e8196ebb"
   },
   "source": [
    "## Benchmark results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4efc1ba3-f5f4-4723-bd6e-bbead42ddd5b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 245
    },
    "id": "4efc1ba3-f5f4-4723-bd6e-bbead42ddd5b",
    "outputId": "a6af51f5-a397-44f6-eed5-b54992071565"
   },
   "outputs": [],
   "source": [
    "#####################################\n",
    "#          Benchmark results        #\n",
    "#####################################\n",
    "required_timers = [\n",
    "    'time_start_embed',\n",
    "    'time_end_embed',\n",
    "    'time_start_cv',\n",
    "    'time_end_cv',\n",
    "    'time_start_model',\n",
    "    'time_end_model',\n",
    "]\n",
    "\n",
    "missing_timers = [name for name in required_timers if name not in globals()]\n",
    "if missing_timers:\n",
    "    raise RuntimeError(\n",
    "        'Missing timer variables for benchmark summary. Run the full notebook in order. '\n",
    "        f'Missing: {missing_timers}'\n",
    "    )\n",
    "\n",
    "# calculating the performances\n",
    "embedding_duration = time_end_embed - time_start_embed\n",
    "cv_duration = time_end_cv - time_start_cv\n",
    "modeling_duration = time_end_model - time_start_model\n",
    "\n",
    "# formating\n",
    "embedding_duration_hms = sec2hms(embedding_duration)\n",
    "cv_duration_hms = sec2hms(cv_duration)\n",
    "modeling_duration_hms = sec2hms(modeling_duration)\n",
    "\n",
    "# printing the embedding, cross validation and modeling performances\n",
    "print(f'Embedding duration : {embedding_duration_hms[0]:.0f}:{embedding_duration_hms[1]:.0f}:{embedding_duration_hms[2]:.3f}')\n",
    "print(f'Cross validation duration : {cv_duration_hms[0]:.0f}:{cv_duration_hms[1]:.0f}:{cv_duration_hms[2]:.3f}')\n",
    "print(f'Best modeling duration : {modeling_duration_hms[0]:.0f}:{modeling_duration_hms[1]:.0f}:{modeling_duration_hms[2]:.3f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fb71ef3-27fb-47bc-8c9b-b1d7dd9aeb17",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4fb71ef3-27fb-47bc-8c9b-b1d7dd9aeb17",
    "outputId": "ef744076-33e1-4832-9545-cc30fa269148"
   },
   "outputs": [],
   "source": [
    "# Save the model state dictionary and reproducibility metadata\n",
    "import json\n",
    "\n",
    "model_path = 'stella_model.pth'\n",
    "config_path = 'stella_model_config.json'\n",
    "\n",
    "torch.save(model.state_dict(), model_path)\n",
    "\n",
    "artifact_config = {\n",
    "    'model_file': model_path,\n",
    "    'embedding_model_name': model_name,\n",
    "    'classifier_architecture': {\n",
    "        'input_size': int(input_size),\n",
    "        'hidden_layers': [3072, 3072, 768],\n",
    "        'dropout': 0.6,\n",
    "        'output_classes': 2,\n",
    "    },\n",
    "    'label_mapping': {\n",
    "        'real': 0,\n",
    "        'fake': 1,\n",
    "    },\n",
    "    'training': {\n",
    "        'best_weight_decay': float(best_weight_decay),\n",
    "        'cv_n_splits': int(cv_n_splits),\n",
    "        'cv_num_epochs': int(cv_num_epochs),\n",
    "        'retrain_num_epochs': int(retrain_num_epochs),\n",
    "        'seed': 42,\n",
    "    },\n",
    "    'dataset': {\n",
    "        'local_csv': 'final_combined_dataset.csv',\n",
    "        'snapshot_hash_file': 'final_combined_dataset.sha256',\n",
    "        'num_rows_after_cleanup': int(len(df)),\n",
    "    },\n",
    "}\n",
    "\n",
    "with open(config_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(artifact_config, f, indent=2)\n",
    "\n",
    "print(f'Model saved: {model_path}')\n",
    "print(f'Metadata saved: {config_path}')\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "covid19_fakenews_stella_training.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
